---
title: "Machine learning project"
author: "Sarah khaled"
date: "July 7, 2019"
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("installr")
library("Hmisc")
library("caret")
```
# Executive summary
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.ce between automatic and manual transmissions"

# Cleaning The data
```{r}
setwd("G:\\datascience\\machinelearning\\project")
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url1, "training.csv")
download.file(url2, "testing.csv")
TrainigData <- read.csv("training.csv")
TestingData <- read.csv("testing.csv")
head(TrainigData)
```
```{r}
#Check NA sum in Columns knowing that the total training size is 19622
column_NAstatus<-colSums(is.na(TrainigData))
column_NAstatus<-data.frame(column_NAstatus)
#by viewing the NA counts there are many column with 19216 NA values which makes these columns not important in our prediction
column_NAstatus[20:40,]

# get rid of columns that have NA values more than 0.5 of total values
NACol<-colSums(is.na(TrainigData))<nrow(TrainigData)/2
TrainigData1<-TrainigData[,NACol]
Comp<-c(ncol(TrainigData),ncol(TrainigData1))
names(Comp)<-c("before","after")
# 67 unnecessary columns have been removed
Comp
# apply for the testing data
TestingData<-TestingData[,NACol]
        
#Git rid of empty columns
Emptycol<-colSums(TrainigData1!="")>nrow(TrainigData1)/2
TrainigData2<-TrainigData1[,Emptycol]
Comp<-c(ncol(TrainigData1),ncol(TrainigData2))
names(Comp)<-c("before","after")
# 33 unnecessary columns  with 19216 empty values from 19622 values have been removed
Comp
TrainigData<-TrainigData2
# apply for the testing data
TestingData<-TestingData[,Emptycol]

# filter out highly correlated predictors
CorrFilter<-preProcess(TrainigData, method = "corr")
TrainigData <- predict(CorrFilter, TrainigData)
TestingData<- predict(CorrFilter, TestingData)

#Remove the non-predictors columns
TrainigData <-TrainigData[,-c(1:7)]
TestingData <-TestingData[,-c(1:7)]

dim(TestingData)
dim(TrainigData)
```

# Pre_Processing
```{r}
#Check for near zero values
preProc<-preProcess(TrainigData, method = "nzv")
#non near zero values found
preProc

```

# Exploratory data analysis
```{r}
#plot the class variabe, It is noted that the arrange of the classes A,B,E,C,D
plot(table(TrainigData$classe),xlab = "class", ylab ="number of users")
# Have a look on the users
table(TrainigData$user_name) 
```

The cleaned data contains 19622 observations and 53 variables.

# Training and prediction
```{r}
#Split the data into training and validation
set.seed(333)
inTrain     <- createDataPartition(TrainigData$classe, p = 0.6, list = FALSE)
Training  <- TrainigData[inTrain,]
validation      <- TrainigData[-inTrain,]

# Using randomforest algorithm and 3 folds
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
trainingModel <- train(classe ~ ., data=Training, method="rf",trControl=controlRF)
trainingModel$finalModel

#Prediction on the validation set
predict<- predict(trainingModel, validation)
cm<-confusionMatrix(validation$classe, predict)

#The validation accuracy
cm$overall['Accuracy']

#Error calculating
Error <- 1 - cm$overall['Accuracy']
Error


#Prediction on the test set
predict<- predict(trainingModel, TestingData)
predict
```















